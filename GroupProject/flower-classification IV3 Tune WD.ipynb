{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1c62db",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.406695,
     "end_time": "2024-04-07T12:58:23.373024",
     "exception": false,
     "start_time": "2024-04-07T12:58:21.966329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491224c-3d18-4dff-a824-6412a44f2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b8a140",
   "metadata": {
    "papermill": {
     "duration": 28.390516,
     "end_time": "2024-04-07T12:58:51.769186",
     "exception": false,
     "start_time": "2024-04-07T12:58:23.378670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 01:34:14.438916: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-24 01:34:14.577099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 01:34:17.817348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.transforms import Compose, Lambda, ToTensor, Normalize, Resize, RandomCrop, TenCrop, RandomHorizontalFlip\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47b41fe9",
   "metadata": {
    "papermill": {
     "duration": 0.09495,
     "end_time": "2024-04-07T12:58:51.869712",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.774762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some settings:\n",
    "# --------------\n",
    "train_files = '/home/msai/mkee004/AI6102/extracted_contents/tfrecords-jpeg-224x224/train/*.tfrec'\n",
    "valid_files = '/home/msai/mkee004/AI6102/extracted_contents/tfrecords-jpeg-224x224/val/*.tfrec'\n",
    "test_files  = '/home/msai/mkee004/AI6102/extracted_contents/tfrecords-jpeg-224x224/test/*.tfrec'\n",
    "device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "wdn_epochs    = 20  \n",
    "lrn_epochs    = 10 \n",
    "batch_size  = 20                                                           \n",
    "num_prints  = 10                                                            \n",
    "train_size  = 12753                                                        \n",
    "print_freq  = train_size // (batch_size * num_prints) + 1                  \n",
    "check_freq  = 1                                                            \n",
    "n_classes = 104\n",
    "base_lr = 1e-3\n",
    "classifier_lr = 3e-3\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cd1a72",
   "metadata": {
    "papermill": {
     "duration": 0.016752,
     "end_time": "2024-04-07T12:58:51.891957",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.875205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to df\n",
    "def tfrecords_to_dataframe(fp, test = False):\n",
    "\n",
    "    def parse(pb, test = False):\n",
    "        d = {'id': tf.io.FixedLenFeature([], tf.string), 'image': tf.io.FixedLenFeature([], tf.string)}\n",
    "        if not test:\n",
    "            d['class'] = tf.io.FixedLenFeature([], tf.int64)\n",
    "        return tf.io.parse_single_example(pb, d)\n",
    "\n",
    "    df = {'id': [], 'img': []} \n",
    "    if not test:\n",
    "        df['lab'] = []\n",
    "    for sample in tf.data.TFRecordDataset(glob.glob(fp)).map(lambda pb: parse(pb, test)):\n",
    "        df['id'].append(sample['id'].numpy().decode('utf-8'))\n",
    "        df['img'].append(sample['image'].numpy())\n",
    "        if not test:\n",
    "            df['lab'].append(sample['class'].numpy())\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd315e6",
   "metadata": {
    "papermill": {
     "duration": 0.013803,
     "end_time": "2024-04-07T12:58:51.910953",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.897150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_images(dataset, n, cols):\n",
    "    rows = n // cols if n % cols == 0 else n // cols + 1\n",
    "    plt.figure(figsize = (2 * cols, 2 * rows))\n",
    "    for i in range(n):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        img, lab = dataset[i]\n",
    "        plt.imshow(img.permute(1, 2, 0).numpy())\n",
    "        plt.title(str(lab))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a8092f",
   "metadata": {
    "papermill": {
     "duration": 0.015483,
     "end_time": "2024-04-07T12:58:51.931845",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.916362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainset(Dataset):\n",
    "    def __init__(self, frac=1):\n",
    "        super().__init__()\n",
    "        self.df = tfrecords_to_dataframe(train_files).sample(frac=frac).reset_index(drop=True)\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Lambda(lambda b: transforms.ToTensor()(Image.open(io.BytesIO(b)))),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            Resize(np.random.randint(300, 641)),\n",
    "            transforms.RandomCrop(300),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #transform = Compose(Resize(np.random.randint(300, 641)))\n",
    "        img = self.transforms(self.df.iloc[idx]['img'])\n",
    "        return img, self.df.iloc[idx]['lab']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240c301f",
   "metadata": {
    "papermill": {
     "duration": 0.016575,
     "end_time": "2024-04-07T12:58:51.953680",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.937105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evalset(Dataset):\n",
    "    def __init__(self, frac=1, test=False):\n",
    "        super().__init__()\n",
    "        files = valid_files if not test else test_files\n",
    "        self.df = tfrecords_to_dataframe(files, test).sample(frac=frac,random_state=0).reset_index(drop=True)\n",
    "        self.transforms = [Compose([\n",
    "            Lambda(lambda b: Image.open(io.BytesIO(b))),\n",
    "            Resize(scale),\n",
    "            TenCrop(300),\n",
    "            Lambda(lambda xs: torch.stack([ToTensor()(x) for x in xs])),\n",
    "            Lambda(lambda xs: torch.stack([Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(x) for x in xs]))\n",
    "        ]) for scale in [372, 568]]\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        sample = self.df.iloc[i]\n",
    "        imgs = torch.stack([t(sample['img']) for t in self.transforms])\n",
    "        return imgs, sample['lab'] if not self.test else sample['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ccb8db",
   "metadata": {
    "papermill": {
     "duration": 0.01422,
     "end_time": "2024-04-07T12:58:51.973089",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.958869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self, n_classes, learnable_modules=('AuxLogits', 'fc')):\n",
    "        super().__init__()\n",
    "        self.inception_v3 = models.inception_v3(pretrained=True)\n",
    "        num_ftrs = self.inception_v3.fc.in_features\n",
    "        self.inception_v3.fc = nn.Linear(num_ftrs, n_classes)\n",
    "\n",
    "        for name, param in self.inception_v3.named_parameters():\n",
    "            if name.split('.')[0] in learnable_modules:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.inception_v3(x)\n",
    "        if isinstance(outputs, torch.Tensor):\n",
    "            return torch.log_softmax(outputs, dim=1)\n",
    "        elif hasattr(outputs, 'logits'):\n",
    "            return torch.log_softmax(outputs.logits, dim=1)\n",
    "        else:\n",
    "            raise TypeError(\"Expected output to have 'logits' attribute\")\n",
    "\n",
    "# Define your model\n",
    "model = InceptionV3(n_classes=104)\n",
    "\n",
    "# Specify different learning rates for different parts of the model\n",
    "# classifier_lr = 0.001\n",
    "# base_lr = 0.0001\n",
    "# weight_decay = 0.0005\n",
    "\n",
    "# Define param_groups for the optimizer\n",
    "param_groups = [\n",
    "    {'params': model.inception_v3.fc.parameters(), 'lr': classifier_lr},\n",
    "    {'params': [p for name, p in model.named_parameters() if 'fc' not in name and p.requires_grad], 'lr': base_lr, 'weight_decay': weight_decay}\n",
    "]\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(params=param_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930f8e21",
   "metadata": {
    "papermill": {
     "duration": 19.727715,
     "end_time": "2024-04-07T12:59:11.706029",
     "exception": false,
     "start_time": "2024-04-07T12:58:51.978314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 01:57:39.413349: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-04-24 01:57:42.953289: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "# valid_loader = DataLoader(Evalset(frac = 0.20), batch_size = 1, num_workers = 2)\n",
    "# test_loader  = DataLoader(Evalset(test = True), batch_size = 1, num_workers = 2)\n",
    "\n",
    "train_loader = DataLoader(Trainset(frac=1), batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "valid_loader = DataLoader(Evalset(frac = 0.20), batch_size = 1, num_workers = 0)\n",
    "#test_loader  = DataLoader(Evalset(test = True,frac=0.1), batch_size = 1, num_workers = 0)\n",
    "\n",
    "\n",
    "# Display some training images and labels:\n",
    "# ----------------------------------------\n",
    "#display_images(train_set, n = 30, cols = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564dc477-3cd8-4577-b9c4-8aa7dca00250",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(Evalset(frac = 0.20), batch_size = 1, num_workers = 0)\n",
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dce1f6-f764-4f23-88d8-7dd1d619737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce10d380",
   "metadata": {
    "papermill": {
     "duration": 3147.63959,
     "end_time": "2024-04-07T13:51:39.922756",
     "exception": false,
     "start_time": "2024-04-07T12:59:12.283166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "--------------------\n",
      "Batch 0, Loss: 4.7466\n",
      "Batch 64, Loss: 3.5487\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "loss_fn = F.nll_loss\n",
    "losses = []\n",
    "valid_f1s = []\n",
    "valid_losses = []\n",
    "max_f1=0\n",
    "last_loss_state = 1\n",
    "model.to(device)\n",
    "name='IV3WD1e_4'\n",
    "# Initialize a file to store training and validation losses and accuracies\n",
    "log_filename = f'{name}_Log.txt'\n",
    "with open(log_filename, 'a') as log_file:\n",
    "    log_file.write(f'\\n{log_filename}\\n')\n",
    "    log_file.write(f'--------------- \\n')\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "\n",
    "for epoch in range(wdn_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{wdn_epochs}\")\n",
    "    print('-' * 20)\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(f'\\n{name} Epoch {epoch + 1}/{wdn_epochs} \\n')\n",
    "        log_file.write(f'--------------- \\n')\n",
    "\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)  # Move inputs to the appropriate device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(x).to(device)  # Forward pass\n",
    "        loss = loss_fn(outputs, y)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)  # Update running loss\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print(f\"Batch {i}, Loss: {loss.item():.4f}\")\n",
    "            with open(log_filename, 'a') as log_file:\n",
    "                log_file.write(f'Batch {i}, Loss: {loss.item():.4f} \\n')\n",
    "            \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)  # Calculate average loss for the epoch\n",
    "    losses.append(epoch_loss)\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(f'Training Loss: {epoch_loss:.4f} \\n')\n",
    "\n",
    "    # Validation phase\n",
    "    if epoch % check_freq == 0:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        valid_true_labs = []\n",
    "        valid_pred_labs = []\n",
    "        \n",
    "        with torch.no_grad():  # Disables gradient calculation\n",
    "            for x, y in valid_loader:\n",
    "                # Assuming the extra dimensions are not needed, and you want to use one of the \"sub-batches\"\n",
    "                # Adjust the slicing based on which part of the data you need\n",
    "                x_corrected = x[:, 0, 0, :, :, :]  # This takes the first \"sub-batch\" and removes extra dimensions\n",
    "                x_corrected = x_corrected.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Now x_corrected should have the shape [1, 3, 300, 300] or similar (adjust based on your needs)\n",
    "                outputs = model(x_corrected)\n",
    "                loss = loss_fn(outputs, y)\n",
    "                running_val_loss += loss.item() * x.size(0)\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                valid_true_labs.extend(y.tolist())\n",
    "                valid_pred_labs.extend(preds.tolist())\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(valid_loader.dataset)\n",
    "        valid_losses.append(epoch_val_loss)\n",
    "        valid_f1 = f1_score(valid_true_labs, valid_pred_labs, average='weighted')\n",
    "        valid_f1s.append(valid_f1)\n",
    "        print(f\"Validation F1: {valid_f1 * 100:.2f}%, Validation Loss: {epoch_val_loss:.4f}\")\n",
    "        with open(log_filename, 'a') as log_file:\n",
    "            log_file.write(f'Validation F1: {valid_f1 * 100:.2f}% \\n')\n",
    "            log_file.write(f'Validation Loss: {epoch_val_loss:.4f} \\n')\n",
    "\n",
    "        # # Save model checkpoint\n",
    "        # torch.save(model.state_dict(), f'./model/{name}_epoch{epoch // check_freq}.pth')\n",
    "\n",
    "        # Save model checkpoint\n",
    "        if epoch_loss < last_loss_state:\n",
    "            last_loss_state = epoch_loss\n",
    "            best_epoch = epoch\n",
    "            print(best_epoch)\n",
    "            if epoch % check_freq == 0:\n",
    "                torch.save(model.state_dict(), f'./ENLR3e_4Tune{epoch // check_freq}.pth')\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "epoch_end_time = time.time()\n",
    "epoch_duration = epoch_end_time - epoch_start_time\n",
    "total_duration = epoch_duration / 60  # Convert to minutes\n",
    "print(f'Total duration: {total_duration} minutes \\n')\n",
    "with open(log_filename, 'a') as log_file:\n",
    "    log_file.write(f'Total duration: {total_duration} minutes \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf75192-0486-46b5-b351-5769c29cf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0withDropOut(nn.Module):\n",
    "    def __init__(self, n_classes, learnable_modules=('classifier.1',), dropout_p=0.15):\n",
    "        super().__init__()\n",
    "        self.efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n",
    "        self.efficientnet_b0.classifier[1] = nn.Linear(self.efficientnet_b0.classifier[1].in_features, n_classes)\n",
    "        self.efficientnet_b0.requires_grad_(False)\n",
    "        \n",
    "        modules = dict(self.efficientnet_b0.named_modules())\n",
    "        for name in learnable_modules:\n",
    "            if name in modules:\n",
    "                modules[name].requires_grad_(True)\n",
    "            else:\n",
    "                raise ValueError(f\"Module name '{name}' not found in the model's named modules.\")\n",
    "        \n",
    "        # Adding dropout layer with specified dropout probability\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying dropout before the classifier\n",
    "        x = self.dropout(x)\n",
    "        return F.log_softmax(self.efficientnet_b0(x), dim=1)\n",
    "\n",
    "modeldropout = nn.DataParallel(EfficientNetB0withDropOut(n_classes = 104, learnable_modules = ('features.5.2', \n",
    "                                                                             'features.6', \n",
    "                                                                             'features.7', \n",
    "                                                                             'features.8', \n",
    "                                                                             'classifier')))\n",
    "modeldropout.to(device)\n",
    "\n",
    "optimizerdropout = torch.optim.Adam(params = [{'params': modeldropout.module.efficientnet_b0.features[5][2].parameters()}, \n",
    "                                       {'params': modeldropout.module.efficientnet_b0.features[6].parameters()}, \n",
    "                                       {'params': modeldropout.module.efficientnet_b0.features[7].parameters()},\n",
    "                                       {'params': modeldropout.module.efficientnet_b0.features[8].parameters()},\n",
    "                                       {'params': modeldropout.module.efficientnet_b0.classifier.parameters(), 'lr': 1e-3}], \n",
    "                             lr = learningRate, \n",
    "                             weight_decay = weightDecay)\n",
    "\n",
    "schedulerdropout = CosineAnnealingLR(optimizerdropout, T_max = n_epochs)\n",
    "\n",
    "losses = []\n",
    "valid_f1s = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{n_epochs}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Training phase\n",
    "    modeldropout.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)  # Move inputs to the appropriate device\n",
    "\n",
    "        optimizerdropout.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = modeldropout(x)  # Forward pass\n",
    "        loss = loss_fn(outputs, y)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizerdropout.step()  # Optimize\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)  # Update running loss\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print(f\"Batch {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)  # Calculate average loss for the epoch\n",
    "    losses.append(epoch_loss)\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    if epoch % check_freq == 0:\n",
    "        modeldropout.eval()  # Set model to evaluation mode\n",
    "        valid_true_labs = []\n",
    "        valid_pred_labs = []\n",
    "        \n",
    "        with torch.no_grad():  # Disables gradient calculation\n",
    "            for x, y in valid_loader:\n",
    "                # Assuming the extra dimensions are not needed, and you want to use one of the \"sub-batches\"\n",
    "                # Adjust the slicing based on which part of the data you need\n",
    "                x_corrected = x[:, 0, 0, :, :, :]  # This takes the first \"sub-batch\" and removes extra dimensions\n",
    "                x_corrected = x_corrected.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Now x_corrected should have the shape [1, 3, 300, 300] or similar (adjust based on your needs)\n",
    "                outputs = modeldropout(x_corrected)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                valid_true_labs.extend(y.tolist())\n",
    "                valid_pred_labs.extend(preds.tolist())\n",
    "\n",
    "        valid_f1 = f1_score(valid_true_labs, valid_pred_labs, average='weighted')\n",
    "        valid_f1s.append(valid_f1)\n",
    "        print(f\"Validation F1: {valid_f1 * 100:.2f}%\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        torch.save(modeldropout.state_dict(), f'./epochdropout{epoch // check_freq}.pth')\n",
    "\n",
    "    # Adjust learning rate\n",
    "    schedulerdropout.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da922722",
   "metadata": {
    "papermill": {
     "duration": 0.949859,
     "end_time": "2024-04-07T13:51:40.912466",
     "exception": false,
     "start_time": "2024-04-07T13:51:39.962607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = np.random.normal(loc=0.5, scale=0.1, size=n_epochs * 10)  # Example losses\n",
    "valid_f1s = np.linspace(0.5, 0.9, n_epochs // check_freq)  # Example F1 scores\n",
    "optimal_epoch = np.argmax(np.array(valid_f1s))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Plot for Training Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(losses)) / (len(losses) / n_epochs), losses, color='tab:blue', linewidth=2, label='Training Loss')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss Over Epochs', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Validation F1 Score\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.plot(np.arange(len(valid_f1s)) * check_freq, valid_f1s, color='tab:green', linewidth=2, label='Validation F1')\n",
    "ax2.vlines(optimal_epoch * check_freq, 0, valid_f1s[optimal_epoch], colors='red', linestyles='dashed', label=f'Optimal Epoch ({optimal_epoch * check_freq})')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Weighted F1 Score', fontsize=12)\n",
    "ax2.set_title('Validation F1 Score Over Epochs', fontsize=14)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "ax2.legend()\n",
    "\n",
    "# Saving and showing the improved plot\n",
    "plt.savefig('improved_plot.png')  # Adjust path as needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa3d30",
   "metadata": {
    "papermill": {
     "duration": 991.576559,
     "end_time": "2024-04-07T14:08:12.526262",
     "exception": false,
     "start_time": "2024-04-07T13:51:40.949703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        ids.append(y[0])\n",
    "        mean_logp = model(x.view(-1, 3, 300, 300).to(device)).mean(dim = 0)\n",
    "        preds.append(torch.argmax(mean_logp).item())\n",
    "submission = pd.DataFrame({'id': ids, 'label': preds})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b77b0",
   "metadata": {
    "papermill": {
     "duration": 0.062528,
     "end_time": "2024-04-07T14:08:12.626053",
     "exception": false,
     "start_time": "2024-04-07T14:08:12.563525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467d271",
   "metadata": {
    "papermill": {
     "duration": 0.055415,
     "end_time": "2024-04-07T14:08:12.719214",
     "exception": false,
     "start_time": "2024-04-07T14:08:12.663799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1243559,
     "sourceId": 21154,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4197.826796,
   "end_time": "2024-04-07T14:08:15.889818",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-07T12:58:18.063022",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
